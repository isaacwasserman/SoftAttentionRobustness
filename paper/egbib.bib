@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@Article{alphafold,
author={Jumper, John
and Evans, Richard
and Pritzel, Alexander
and Green, Tim
and Figurnov, Michael
and Ronneberger, Olaf
and Tunyasuvunakool, Kathryn
and Bates, Russ
and {\v{Z}}{\'i}dek, Augustin
and Potapenko, Anna
and Bridgland, Alex
and Meyer, Clemens
and Kohl, Simon A. A.
and Ballard, Andrew J.
and Cowie, Andrew
and Romera-Paredes, Bernardino
and Nikolov, Stanislav
and Jain, Rishub
and Adler, Jonas
and Back, Trevor
and Petersen, Stig
and Reiman, David
and Clancy, Ellen
and Zielinski, Michal
and Steinegger, Martin
and Pacholska, Michalina
and Berghammer, Tamas
and Bodenstein, Sebastian
and Silver, David
and Vinyals, Oriol
and Senior, Andrew W.
and Kavukcuoglu, Koray
and Kohli, Pushmeet
and Hassabis, Demis},
title={Highly accurate protein structure prediction with AlphaFold},
journal={Nature},
year={2021},
month={Aug},
day={01},
volume={596},
number={7873},
pages={583-589},
abstract={Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1--4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence---the structure prediction component of the `protein folding problem'8---has been an important open research problem for more than 50 years9. Despite recent progress10--14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
issn={1476-4687},
doi={10.1038/s41586-021-03819-2},
url={https://doi.org/10.1038/s41586-021-03819-2}
}

@Article{muzero,
author={Schrittwieser, Julian
and Antonoglou, Ioannis
and Hubert, Thomas
and Simonyan, Karen
and Sifre, Laurent
and Schmitt, Simon
and Guez, Arthur
and Lockhart, Edward
and Hassabis, Demis
and Graepel, Thore
and Lillicrap, Timothy
and Silver, David},
title={Mastering Atari, Go, chess and shogi by planning with a learned model},
journal={Nature},
year={2020},
month={Dec},
day={01},
volume={588},
number={7839},
pages={604-609},
abstract={Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3---the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4---the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi---canonical environments for high-performance planning---the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.},
issn={1476-4687},
doi={10.1038/s41586-020-03051-4},
url={https://doi.org/10.1038/s41586-020-03051-4}
}

@article{medical-cnn-survey, 
year = {2021}, 
title = {{Convolutional neural networks for medical image analysis: State-of-the-art, comparisons, improvement and perspectives}}, 
author = {Yu, Hang and Yang, Laurence T. and Zhang, Qingchen and Armstrong, David and Deen, M. Jamal}, 
journal = {Neurocomputing}, 
issn = {0925-2312}, 
doi = {10.1016/j.neucom.2020.04.157}, 
abstract = {{Convolutional neural networks, are one of the most representative deep learning models. CNNs were extensively used in many aspects of medical image analysis, allowing for great progress in computer-aided diagnosis in recent years. In this paper, we provide a survey on convolutional neural networks in medical image analysis. First, we review the commonly used CNNs in medical image processing, including AlexNet, GoogleNet, ResNet, R-CNN, and FCNN. Then, we present an overview of the use of CNNs, for image classification, segmentation, detection, and other tasks such as registration, content-based image retrieval, image generation and enhancement, in some typical medical diagnosis areas such as brain, breast, and abdominal. Finally, we discuss the remaining challenges of CNNs in medical image analysis, and accordingly we present some ideas for future research directions.}}, 
pages = {92--110}, 
volume = {444}, 
keywords = {}
}

@Article{AI-CDSS,
author={Bleher, Hannah
and Braun, Matthias},
title={Diffused responsibility: attributions of responsibility in the use of AI-driven clinical decision support systems},
journal={AI and Ethics},
year={2022},
month={Jan},
day={24},
abstract={Good decision-making is a complex endeavor, and particularly so in a health context. The possibilities for day-to-day clinical practice opened up by AI-driven clinical decision support systems (AI-CDSS) give rise to fundamental questions around responsibility. In causal, moral and legal terms the application of AI-CDSS is challenging existing attributions of responsibility. In this context, responsibility gaps are often identified as main problem. Mapping out the changing dynamics and levels of attributing responsibility, we argue in this article that the application of AI-CDSS causes diffusions of responsibility with respect to a causal, moral, and legal dimension. Responsibility diffusion describes the situation where multiple options and several agents can be considered for attributing responsibility. Using the example of an AI-driven `digital tumor board', we illustrate how clinical decision-making is changed and diffusions of responsibility take place. Not denying or attempting to bridge responsibility gaps, we argue that dynamics and ambivalences are inherent in responsibility, which is based on normative considerations such as avoiding experiences of disregard and vulnerability of human life, which are inherently accompanied by a moment of uncertainty, and is characterized by revision openness. Against this background and to avoid responsibility gaps, the article concludes with suggestions for managing responsibility diffusions in clinical decision-making with AI-CDSS.},
issn={2730-5961},
doi={10.1007/s43681-022-00135-x},
url={https://doi.org/10.1007/s43681-022-00135-x}
}

@ARTICLE{NIH-AI,
  title     = "Artificial intelligence: How is it changing medical sciences and
               its future?",
  author    = "Basu, Kanadpriya and Sinha, Ritwik and Ong, Aihui and Basu,
               Treena",
  abstract  = "Artificially intelligent computer systems are used extensively
               in medical sciences. Common applications include diagnosing
               patients, end-to-end drug discovery and development, improving
               communication between physician and patient, transcribing
               medical documents, such as prescriptions, and remotely treating
               patients. While computer systems often execute tasks more
               efficiently than humans, more recently, state-of-the-art
               computer algorithms have achieved accuracies which are at par
               with human experts in the field of medical sciences. Some
               speculate that it is only a matter of time before humans are
               completely replaced in certain roles within the medical
               sciences. The motivation of this article is to discuss the ways
               in which artificial intelligence is changing the landscape of
               medical science and to separate hype from reality.",
  journal   = "Indian J. Dermatol.",
  publisher = "Medknow",
  volume    =  65,
  number    =  5,
  pages     = "365--370",
  month     =  sep,
  year      =  2020,
  keywords  = "Artificial intelligence; deep convolutional neural network;
               medical use",
  language  = "en"
}

@ARTICLE{Med-XAI,
  author={Tjoa, Erico and Guan, Cuntai},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI}, 
  year={2021},
  volume={32},
  number={11},
  pages={4793-4813},
  doi={10.1109/TNNLS.2020.3027314}}

@inproceedings{Intro-Adv,
  title	= {Intriguing properties of neural networks},
  author	= {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
  year	= {2014},
  URL	= {http://arxiv.org/abs/1312.6199},
  booktitle	= {International Conference on Learning Representations}
}

@InProceedings{Natural-Adv,
    author    = {Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
    title     = {Natural Adversarial Examples},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {15262-15271}
}

@InProceedings{ResNet,
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title = {Deep Residual Learning for Image Recognition},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2016}
}

@article{RobustVsAccuracy, 
year = {2018}, 
title = {{Robustness May Be at Odds with Accuracy}}, 
author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander}, 
journal = {arXiv}, 
eprint = {1805.12152}, 
abstract = {{We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the representations learned by robust models tend to align better with salient data characteristics and human perception.}}, 
keywords = {}
}

@article{Finlayson, 
year = {2019}, 
title = {{Adversarial attacks on medical machine learning}}, 
author = {Finlayson, Samuel G. and Bowers, John D. and Ito, Joichi and Zittrain, Jonathan L. and Beam, Andrew L. and Kohane, Isaac S.}, 
journal = {Science}, 
issn = {0036-8075}, 
doi = {10.1126/science.aaw4399}, 
pmid = {30898923}, 
abstract = {{Emerging vulnerabilities demand new conversations}}, 
pages = {1287--1289}, 
number = {6433}, 
volume = {363}, 
keywords = {}
}

@article{Paschali, 
year = {2018}, 
title = {{Medical Image Computing and Computer Assisted Intervention – MICCAI 2018, 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part I}}, 
author = {Paschali, Magdalini and Conjeti, Sailesh and Navarro, Fernando and Navab, Nassir}, 
journal = {Lecture Notes in Computer Science}, 
issn = {0302-9743}, 
doi = {10.1007/978-3-030-00928-1\_56}, 
abstract = {{In this paper, for the first time, we propose an evaluation method for deep learning models that assesses the performance of a model not only in an unseen test scenario, but also in extreme cases of noise, outliers and ambiguous input data. To this end, we utilize adversarial examples, images that fool machine learning models, while looking imperceptibly different from original data, as a measure to evaluate the robustness of a variety of medical imaging models. Through extensive experiments on skin lesion classification and whole brain segmentation with state-of-the-art networks such as Inception and UNet, we show that models that achieve comparable performance regarding generalizability may have significant variations in their perception of the underlying data manifold, leading to an extensive performance gap in their robustness.}}, 
pages = {493--501}, 
keywords = {}
}

@article{MaNiu, 
year = {2021}, 
title = {{Understanding adversarial attacks on deep learning based medical image analysis systems}}, 
author = {Ma, Xingjun and Niu, Yuhao and Gu, Lin and Wang, Yisen and Zhao, Yitian and Bailey, James and Lu, Feng}, 
journal = {Pattern Recognition}, 
issn = {0031-3203}, 
doi = {10.1016/j.patcog.2020.107332}, 
eprint = {1907.10456}, 
abstract = {{Deep neural networks (DNNs) have become popular for medical image analysis tasks like cancer diagnosis and lesion detection. However, a recent study demonstrates that medical deep learning systems can be compromised by carefully-engineered adversarial examples/attacks with small imperceptible perturbations. This raises safety concerns about the deployment of these systems in clinical settings. In this paper, we provide a deeper understanding of adversarial examples in the context of medical images. We find that medical DNN models can be more vulnerable to adversarial attacks compared to models for natural images, according to two different viewpoints. Surprisingly, we also find that medical adversarial attacks can be easily detected, i.e., simple detectors can achieve over 98\% detection AUC against state-of-the-art attacks, due to fundamental feature differences compared to normal examples. We believe these findings may be a useful basis to approach the design of more explainable and secure medical deep learning systems.}}, 
pages = {107332}, 
volume = {110}, 
keywords = {}
}

@article{Madry, 
year = {2017}, 
title = {{Towards Deep Learning Models Resistant to Adversarial Attacks}}, 
author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian}, 
journal = {arXiv}, 
eprint = {1706.06083}, 
abstract = {{Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.}}, 
keywords = {}
}

@inproceedings{Huang, 
year = {2021}, 
author = {Huang, Hanxun and Wang, Yisen and Erfani, Sarah Monazam and Gu, Quanquan and Bailey, James and Ma, Xingjun}, 
title = {{Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks}}, 
booktitle = {Advances in Neural Information Processing Systems}, 
url = {https://proceedings.neurips.cc/paper/2021/file/2bd7f907b7f5b6bbd91822c0c7b835f6-Paper.pdf}, 
abstract = {{Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks. A range of defense methods have been proposed to train adversarially robust DNNs, among which adversarial training has demonstrated promising results. However, despite preliminary understandings developed for adversarial training, it is still not clear, from the architectural perspective, what configurations can lead to more robust DNNs. In this paper, we address this gap via a comprehensive investigation on the impact of network width and depth on the robustness of adversarially trained DNNs. Specifically, we make the following key observations: 1) more parameters (higher model capacity) does not necessarily help adversarial robustness; 2) reducing capacity at the last stage (the last group of blocks) of the network can actually improve adversarial robustness; and 3) under the same parameter budget, there exists an optimal architectural configuration for adversarial robustness. We also provide a theoretical analysis explaning why such network configuration can help robustness. These architectural insights can help design adversarially robust DNNs. Code is available at \textbackslashurl\{https://github.com/HanxunH/RobustWRN\}.}}, 
pages = {5545--5559}, 
volume = {34}, 
series = {arXiv}, 
publisher = {Curran Associates, Inc.}, 
keywords = {}
}

@article{RACL, 
year = {2020}, 
title = {{Adversarially Robust Neural Architectures}}, 
author = {Dong, Minjing and Li, Yanxi and Wang, Yunhe and Xu, Chang}, 
journal = {arXiv}, 
eprint = {2009.00902}, 
abstract = {{Deep Neural Network (DNN) are vulnerable to adversarial attack. Existing methods are devoted to developing various robust training strategies or regularizations to update the weights of the neural network. But beyond the weights, the overall structure and information flow in the network are explicitly determined by the neural architecture, which remains unexplored. This paper thus aims to improve the adversarial robustness of the network from the architecture perspective with NAS framework. We explore the relationship among adversarial robustness, Lipschitz constant, and architecture parameters and show that an appropriate constraint on architecture parameters could reduce the Lipschitz constant to further improve the robustness. For NAS framework, all the architecture parameters are equally treated when the discrete architecture is sampled from supernet. However, the importance of architecture parameters could vary from operation to operation or connection to connection, which is not explored and might reduce the confidence of robust architecture sampling. Thus, we propose to sample architecture parameters from trainable multivariate log-normal distributions, with which the Lipschitz constant of entire network can be approximated using a univariate log-normal distribution with mean and variance related to architecture parameters. Compared with adversarially trained neural architectures searched by various NAS algorithms as well as efficient human-designed models, our algorithm empirically achieves the best performance among all the models under various attacks on different datasets.}}, 
keywords = {}
}

@article{AdvRush, 
year = {2021}, 
title = {{AdvRush: Searching for Adversarially Robust Neural Architectures}}, 
author = {Mok, Jisoo and Na, Byunggook and Choe, Hyeokjun and Yoon, Sungroh}, 
journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
doi = {10.1109/iccv48922.2021.01210}, 
abstract = {{Deep neural networks continue to awe the world with their remarkable performance. Their predictions, however, are prone to be corrupted by adversarial examples that are imperceptible to humans. Current efforts to improve the robustness of neural networks against adversarial examples are focused on developing robust training methods, which update the weights of a neural network in a more robust direction. In this work, we take a step beyond training of the weight parameters and consider the problem of designing an adversarially robust neural architecture with high intrinsic robustness. We propose AdvRush, a novel adversarial robustness-aware neural architecture search algorithm, based upon a finding that independent of the training method, the intrinsic robustness of a neural network can be represented with the smoothness of its input loss landscape. Through a regularizer that favors a candidate architecture with a smoother input loss landscape, AdvRush successfully discovers an adversarially robust neural architecture. Along with a comprehensive theoretical motivation for AdvRush, we conduct an extensive amount of experiments to demonstrate the efficacy of AdvRush on various benchmark datasets. Notably, on CIFAR-10, AdvRush achieves 55.91\% robust accuracy under FGSM attack after standard training and 50.04\% robust accuracy under AutoAttack after 7-step PGD adversarial training.}}, 
pages = {12302--12312}, 
volume = {00}, 
keywords = {}
}
